// scripts/kb/sync-central-vector-store-vs-files.ts
//
// Sync VS ingestion files (generated by build-vs-files-from-pages.ts) into a CENTRAL vector store,
// using manifest.json for attributes, and upserting kb_document_files in Supabase.
//
// Usage:
//   tsx scripts/kb/sync-central-vector-store-vs-files.ts <vsFilesDir> <centralVectorStoreId>
//
// Example:
//   tsx scripts/kb/sync-central-vector-store-vs-files.ts scripts/kb_scrapes/all/vs_files vs_abc123
//
// Notes:
// - Expects <vsFilesDir>/manifest.json produced by build-vs-files-from-pages.ts.
// - Uses content_hash for idempotency: if unchanged, skips upload.
// - Deletes (detaches) active rows in kb_document_files that are not present in manifest (for the kb_slugs present).
//
// Env required:
//   OPENAI_API_KEY
//   NEXT_PUBLIC_SUPABASE_URL
//   SUPABASE_SERVICE_ROLE_KEY

import fs from "node:fs";
import path from "node:path";
import crypto from "node:crypto";
import dotenv from "dotenv";
import OpenAI from "openai";
import pLimit from "p-limit";
import { createClient, SupabaseClient } from "@supabase/supabase-js";

dotenv.config({ path: path.resolve(process.cwd(), ".env.local") });
dotenv.config({ path: path.resolve(process.cwd(), ".env") });

const CONCURRENCY = 3;

type ManifestEntry = {
  page_path: string;
  vs_file_path: string;
  attributes: Record<string, string | number | boolean>;
};

type KBDocFileRow = {
  kb_slug: string;
  doc_id: string;
  section_key: string;

  canonical_url: string | null;
  title: string | null;
  site: string | null;

  uni: string | null;
  corpus: string | null;
  year: number | null;

  vector_store_id: string | null;
  file_id: string | null;
  vector_store_file_id: string | null;

  content_hash: string | null;
  last_fetched_at: string;
  is_active: boolean;
};

function usageAndExit(): never {
  console.error(
    "Usage: tsx scripts/kb/sync-central-vector-store-vs-files.ts <vsFilesDir> <centralVectorStoreId>\n" +
      "Example: tsx scripts/kb/sync-central-vector-store-vs-files.ts scripts/kb_scrapes/all/vs_files vs_abc123"
  );
  process.exit(1);
}

function clampAttr(s: unknown, maxLen: number): string {
  const v = String(s ?? "").trim();
  if (!v) return "";
  if (v.length <= maxLen) return v;

  const hash = crypto.createHash("sha1").update(v).digest("hex").slice(0, 10);
  const suffix = `...#${hash}`;
  const keep = Math.max(0, maxLen - suffix.length);
  const out = v.slice(0, keep) + suffix;
  return out.length <= maxLen ? out : out.slice(0, maxLen);
}

function makeSupabaseAdminClient(): SupabaseClient {
  const url = process.env.NEXT_PUBLIC_SUPABASE_URL;
  const serviceKey = process.env.SUPABASE_SERVICE_ROLE_KEY;
  if (!url) throw new Error("Missing NEXT_PUBLIC_SUPABASE_URL");
  if (!serviceKey) throw new Error("Missing SUPABASE_SERVICE_ROLE_KEY");
  return createClient(url, serviceKey, { auth: { persistSession: false } });
}

async function fetchAllDocFileRows(supabase: SupabaseClient, kbSlug: string): Promise<KBDocFileRow[]> {
  const rows: KBDocFileRow[] = [];
  const pageSize = 1000;
  let from = 0;

  while (true) {
    const to = from + pageSize - 1;
    const { data, error } = await supabase
      .from("kb_document_files")
      .select("*")
      .eq("kb_slug", kbSlug)
      .range(from, to);

    if (error) throw error;
    if (!data || data.length === 0) break;

    rows.push(...(data as KBDocFileRow[]));
    if (data.length < pageSize) break;
    from += pageSize;
  }

  return rows;
}

function readManifest(vsFilesDir: string): ManifestEntry[] {
  const manifestPath = path.join(vsFilesDir, "manifest.json");
  if (!fs.existsSync(manifestPath)) throw new Error(`manifest.json not found: ${manifestPath}`);
  const raw = fs.readFileSync(manifestPath, "utf-8");
  const parsed = JSON.parse(raw);
  if (!Array.isArray(parsed)) throw new Error("manifest.json must be an array");
  return parsed as ManifestEntry[];
}

function keyOf(attrs: Record<string, any>): string {
  const kb_slug = String(attrs.kb_slug ?? "");
  const doc_id = String(attrs.doc_id ?? "");
  const section_key = String(attrs.section_key ?? "");
  if (!kb_slug || !doc_id || !section_key) {
    throw new Error(`Missing required attributes for key: kb_slug/doc_id/section_key`);
  }
  return `${kb_slug}:${doc_id}:${section_key}`;
}

async function uploadAndAttach(
  openai: OpenAI,
  vectorStoreId: string,
  filePath: string,
  attributes: Record<string, any>
): Promise<{ file_id: string; vector_store_file_id: string }> {
  const created = await openai.files.create({
    file: fs.createReadStream(filePath),
    purpose: "assistants",
  });

  const vsFile = await openai.vectorStores.files.create(vectorStoreId, {
    file_id: created.id,
    attributes,
  });

  return { file_id: created.id, vector_store_file_id: vsFile.id };
}

async function deleteVSFile(
  openai: OpenAI,
  vectorStoreId: string,
  vectorStoreFileId: string | null | undefined
) {
  if (!vectorStoreFileId) return;
  try {
    await openai.vectorStores.files.delete(vectorStoreFileId, { vector_store_id: vectorStoreId });
  } catch (e: any) {
    console.warn(`Warning: failed to delete vector store file ${vectorStoreFileId}:`, e?.message ?? e);
  }
}

async function upsertDocFileRow(
  supabase: SupabaseClient,
  row: Omit<KBDocFileRow, "last_fetched_at">
) {
  const payload = { ...row, last_fetched_at: new Date().toISOString() };
  const { error } = await supabase
    .from("kb_document_files")
    .upsert(payload, { onConflict: "kb_slug,doc_id,section_key" });

  if (error) throw error;
}

async function markDocFileInactive(
  supabase: SupabaseClient,
  kb_slug: string,
  doc_id: string,
  section_key: string
) {
  const { error } = await supabase
    .from("kb_document_files")
    .update({ is_active: false, last_fetched_at: new Date().toISOString() })
    .eq("kb_slug", kb_slug)
    .eq("doc_id", doc_id)
    .eq("section_key", section_key);

  if (error) throw error;
}

function sanitizeAttributes(attrs: Record<string, any>): Record<string, any> {
  // Keep attributes tight and within typical limits.
  // NOTE: If you later decide to drop canonical_url from attributes, remove it here.
  const out: Record<string, any> = {
    uni: String(attrs.uni ?? ""),
    corpus: String(attrs.corpus ?? ""),
    year: Number(attrs.year ?? 2026),
    kb_slug: String(attrs.kb_slug ?? ""),
    doc_id: String(attrs.doc_id ?? ""),
    site: String(attrs.site ?? ""),
    section_key: String(attrs.section_key ?? ""),
    canonical_url: clampAttr(attrs.canonical_url ?? "", 512),
    content_hash: String(attrs.content_hash ?? ""),
  };

  // Remove empties so you donâ€™t attach noisy attributes
  for (const k of Object.keys(out)) {
    const v = out[k];
    if (v === "" || v === null || v === undefined || (typeof v === "number" && Number.isNaN(v))) {
      delete out[k];
    }
  }
  return out;
}

async function main() {
  const [, , vsFilesDirArg, centralVectorStoreId] = process.argv;
  if (!vsFilesDirArg || !centralVectorStoreId) usageAndExit();

  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) throw new Error("Missing OPENAI_API_KEY");

  const vsFilesDir = path.resolve(process.cwd(), vsFilesDirArg);
  if (!fs.existsSync(vsFilesDir)) throw new Error(`vsFilesDir not found: ${vsFilesDir}`);

  const openai = new OpenAI({ apiKey });
  const supabase = makeSupabaseAdminClient();

  // Validate store exists
  const store = await openai.vectorStores.retrieve(centralVectorStoreId);
  console.log(`Using CENTRAL vector store: ${store.id} (status=${store.status})`);

  const manifest = readManifest(vsFilesDir);
  console.log(`Loaded manifest entries: ${manifest.length}`);

  // Normalize paths and attributes
  const desired = new Map<string, { filePath: string; attrs: Record<string, any> }>();
  const kbSlugsInRun = new Set<string>();

  for (const entry of manifest) {
    const filePathAbs = path.isAbsolute(entry.vs_file_path)
      ? entry.vs_file_path
      : path.resolve(process.cwd(), entry.vs_file_path);

    if (!fs.existsSync(filePathAbs)) {
      throw new Error(`VS file not found: ${filePathAbs}`);
    }

    const attrs = sanitizeAttributes(entry.attributes ?? {});
    const k = keyOf(attrs);

    desired.set(k, { filePath: filePathAbs, attrs });
    kbSlugsInRun.add(String(attrs.kb_slug ?? ""));
  }

  // Fetch existing rows per kb_slug (only the kb_slugs present in this manifest)
  const existingRows: KBDocFileRow[] = [];
  for (const kbSlug of kbSlugsInRun) {
    if (!kbSlug) continue;
    const rows = await fetchAllDocFileRows(supabase, kbSlug);
    existingRows.push(...rows);
  }

  const existingByKey = new Map(existingRows.map(r => [`${r.kb_slug}:${r.doc_id}:${r.section_key}`, r]));
  const activeExistingKeys = new Set(existingRows.filter(r => r.is_active).map(r => `${r.kb_slug}:${r.doc_id}:${r.section_key}`));

  // Plan
  const toAdd: Array<{ key: string; filePath: string; attrs: Record<string, any> }> = [];
  const toUpdate: Array<{ key: string; filePath: string; attrs: Record<string, any>; existing: KBDocFileRow }> = [];
  const toSkip: Array<{ key: string }> = [];
  const toDelete: KBDocFileRow[] = [];

  for (const [k, v] of desired.entries()) {
    const ex = existingByKey.get(k);
    if (!ex) {
      toAdd.push({ key: k, filePath: v.filePath, attrs: v.attrs });
      continue;
    }

    const desiredHash = String(v.attrs.content_hash ?? "");
    const existingHash = String(ex.content_hash ?? "");

    // Skip if active, same hash, and already attached to this central store
    if (ex.is_active && desiredHash && desiredHash === existingHash && ex.vector_store_id === centralVectorStoreId && ex.file_id) {
      toSkip.push({ key: k });
      continue;
    }

    toUpdate.push({ key: k, filePath: v.filePath, attrs: v.attrs, existing: ex });
  }

  for (const k of activeExistingKeys) {
    if (!desired.has(k)) {
      const ex = existingByKey.get(k);
      if (ex && ex.vector_store_id === centralVectorStoreId) {
        toDelete.push(ex);
      }
    }
  }

  console.log(
    `\nPlan (central store=${centralVectorStoreId}):\n` +
      `  Add:    ${toAdd.length}\n` +
      `  Update: ${toUpdate.length}\n` +
      `  Delete: ${toDelete.length}\n` +
      `  Skip:   ${toSkip.length}\n`
  );

  const limit = pLimit(CONCURRENCY);

  // 1) Delete (detach) rows missing from manifest
  await Promise.all(
    toDelete.map(ex =>
      limit(async () => {
        await deleteVSFile(openai, centralVectorStoreId, ex.vector_store_file_id);
        await markDocFileInactive(supabase, ex.kb_slug, ex.doc_id, ex.section_key);
      })
    )
  );

  // 2) Update changed rows
  await Promise.all(
    toUpdate.map(u =>
      limit(async () => {
        // Detach old attachment if we have it and it was attached to central store
        if (u.existing.vector_store_id === centralVectorStoreId) {
          await deleteVSFile(openai, centralVectorStoreId, u.existing.vector_store_file_id);
        }

        const { file_id, vector_store_file_id } = await uploadAndAttach(
          openai,
          centralVectorStoreId,
          u.filePath,
          u.attrs
        );

        const kb_slug = String(u.attrs.kb_slug);
        const doc_id = String(u.attrs.doc_id);
        const section_key = String(u.attrs.section_key);

        await upsertDocFileRow(supabase, {
          kb_slug,
          doc_id,
          section_key,

          canonical_url: String(u.attrs.canonical_url ?? "") || null,
          title: null,
          site: String(u.attrs.site ?? "") || null,

          uni: String(u.attrs.uni ?? "") || null,
          corpus: String(u.attrs.corpus ?? "") || null,
          year: Number(u.attrs.year ?? 2026),

          vector_store_id: centralVectorStoreId,
          file_id,
          vector_store_file_id,

          content_hash: String(u.attrs.content_hash ?? "") || null,
          is_active: true,
        } as Omit<KBDocFileRow, "last_fetched_at">);
      })
    )
  );

  // 3) Add new rows
  await Promise.all(
    toAdd.map(a =>
      limit(async () => {
        const { file_id, vector_store_file_id } = await uploadAndAttach(
          openai,
          centralVectorStoreId,
          a.filePath,
          a.attrs
        );

        const kb_slug = String(a.attrs.kb_slug);
        const doc_id = String(a.attrs.doc_id);
        const section_key = String(a.attrs.section_key);

        await upsertDocFileRow(supabase, {
          kb_slug,
          doc_id,
          section_key,

          canonical_url: String(a.attrs.canonical_url ?? "") || null,
          title: null,
          site: String(a.attrs.site ?? "") || null,

          uni: String(a.attrs.uni ?? "") || null,
          corpus: String(a.attrs.corpus ?? "") || null,
          year: Number(a.attrs.year ?? 2026),

          vector_store_id: centralVectorStoreId,
          file_id,
          vector_store_file_id,

          content_hash: String(a.attrs.content_hash ?? "") || null,
          is_active: true,
        } as Omit<KBDocFileRow, "last_fetched_at">);
      })
    )
  );

  const finalStore = await openai.vectorStores.retrieve(centralVectorStoreId);
  console.log(
    "\nSync complete.\nVector store:",
    finalStore.id,
    "status:",
    finalStore.status,
    "file_counts:",
    finalStore.file_counts
  );
  console.log(`Summary: added=${toAdd.length}, updated=${toUpdate.length}, deleted=${toDelete.length}, skipped=${toSkip.length}`);
}

main().catch((e) => {
  console.error(e);
  process.exit(1);
});